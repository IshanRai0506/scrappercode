{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abccf017-d0db-401c-b9e1-2e820df171b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy_selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspiders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrawlSpider  \u001b[38;5;66;03m# CrawlSpider class for crawling multiple pages\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m  \u001b[38;5;66;03m# Regular expressions library to identify patterns (emails in this case)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscrapy_selenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SeleniumRequest  \u001b[38;5;66;03m# Scrapy-Selenium to handle JavaScript websites\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By  \u001b[38;5;66;03m# Used for finding elements on a page\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC  \u001b[38;5;66;03m# Used to wait until page loads\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy_selenium'"
     ]
    }
   ],
   "source": [
    "# Importing required libraries\n",
    "import scrapy  # Scrapy framework for scraping\n",
    "from scrapy.spiders import CrawlSpider  # CrawlSpider class for crawling multiple pages\n",
    "import re  # Regular expressions library to identify patterns (emails in this case)\n",
    "from scrapy_selenium import SeleniumRequest  # Scrapy-Selenium to handle JavaScript websites\n",
    "from selenium.webdriver.common.by import By  # Used for finding elements on a page\n",
    "from selenium.webdriver.support import expected_conditions as EC  # Used to wait until page loads\n",
    "\n",
    "# Defining the class that will handle the email extraction\n",
    "class EmailExtractor(CrawlSpider):\n",
    "    # Give your spider a name so you can run it with this name later\n",
    "    name = 'email_extractor'\n",
    "\n",
    "    # This is the constructor, it gets called when you create an object from the class\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Call the parent class's constructor\n",
    "        super(EmailExtractor, self).__init__(*args, **kwargs)\n",
    "        # List to store the found email addresses\n",
    "        self.email_list = []\n",
    "        # Add the websites you want to scrape for emails here\n",
    "        self.urls = [\"https://vitbhopal.ac.in/\"\n",
    "        ]\n",
    "\n",
    "    # This function starts the scraping process\n",
    "    def start_requests(self):\n",
    "        # Loop through each URL in the list of websites\n",
    "        for url in self.urls:\n",
    "            # For each URL, create a request to visit the page using Selenium to handle JavaScript\n",
    "            yield SeleniumRequest(\n",
    "                url=url,  # Website URL\n",
    "                callback=self.parse,  # Once the page loads, call the 'parse' function to process it\n",
    "                wait_until=EC.presence_of_element_located((By.TAG_NAME, \"html\")),  # Wait until the page is fully loaded\n",
    "                dont_filter=True  # Allow revisiting the same domain if needed\n",
    "            )\n",
    "\n",
    "    # This function processes the page and extracts the emails\n",
    "    def parse(self, response):\n",
    "        # Define a regular expression pattern to match email addresses\n",
    "        EMAIL_REGEX = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
    "        \n",
    "        # Use 're.finditer' to find all the email addresses on the page\n",
    "        emails = re.finditer(EMAIL_REGEX, str(response.text))\n",
    "        \n",
    "        # Loop through the found emails and add them to the email list\n",
    "        for email in emails:\n",
    "            self.email_list.append(email.group())  # 'email.group()' gives the full email text\n",
    "\n",
    "        # Yield (return) each unique email found, removing duplicates using 'set()'\n",
    "        for email in set(self.email_list):\n",
    "            yield {\n",
    "                \"email\": email  # Return the email in dictionary format, where 'email' is the key\n",
    "            }\n",
    "\n",
    "        # Clear the email list after processing each page to avoid duplicating emails across pages\n",
    "        self.email_list.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6dd75-3ac2-4a44-b6b4-1adb8e5bd962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
